{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_spec = {'dataset_1': {'original_dims': 30,\n",
    "                              'output_dims': 2,\n",
    "                              'max_additional_dims': 50,\n",
    "                              'mean_val': [list(2 * np.round(np.random.randn(30), decimals=3)),\n",
    "                                            list(-2 * np.round(np.random.randn(30), decimals=3))],\n",
    "                              'std_val': [list(0.5 * np.ones(30)),\n",
    "                                           list(0.5 * np.ones(30))],\n",
    "                              'noise': 'gaussian',\n",
    "                              'noise_mean': 0.,\n",
    "                              'noise_sigma': 0.5,\n",
    "                              'n_samples_per_class': 5000\n",
    "                             }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Hyperparameters(object):\n",
    "    \"\"\" Add hyper-parameters in init so when you read a json, it will get updated as your latest code. \"\"\"\n",
    "    def __init__(self,\n",
    "                 learning_rate=5e-2,\n",
    "                 architecture=None,\n",
    "                 epochs=500,\n",
    "                 batch_size=10,\n",
    "                 loss='cross_entropy',\n",
    "                 optimizer='sgd',\n",
    "                 lr_at_plateau=True,\n",
    "                 reduction_factor=None,\n",
    "                 validation_check=True):\n",
    "        \"\"\"\n",
    "        :param learning_rate: float, the initial value for the learning rate\n",
    "        :param architecture: str, the architecture types\n",
    "        :param epochs: int, the number of epochs we want to train\n",
    "        :param batch_size: int, the dimension of the batch size\n",
    "        :param loss: str, loss type, cross entropy or square loss\n",
    "        :param optimizer: str, the optimizer type.\n",
    "        :param lr_at_plateau: bool, protocol to decrease the learning rate.\n",
    "        :param reduction_factor, int, the factor which we use to reduce the learning rate.\n",
    "        :param validation_check: bool, if we want to keep track of validation loss as a stopping criterion.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.architecture = architecture\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_at_plateau = lr_at_plateau\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.validation_check = validation_check\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\" Here we save the dataset specific related to each experiment. The name of the dataset,\n",
    "    the scenario, if we modify the original dataset, and the dimensions of the input.\n",
    "    This is valid for the modified_MNIST_dataset, verify if it is going to be valid next\"\"\"\n",
    "    # TODO: add output_dims\n",
    "    def __init__(self,\n",
    "                 scenario=1,\n",
    "                 original_dims=30,\n",
    "                 output_dims=2,\n",
    "                 additional_dims=2,\n",
    "                 mean_val=None,\n",
    "                 std_val=None,\n",
    "                 noise='gaussian',\n",
    "                 noise_mean=0.,\n",
    "                 noise_sigma=0.5,\n",
    "                 n_training=10,\n",
    "                 redundancy_amount=None):\n",
    "        \"\"\"\n",
    "        :param scenario: int, the learning paradigm\n",
    "        :param original_dims: int, name of the folder of the experiments\n",
    "        :param output_dims: int, dimensionality of the output\n",
    "        :param additional_dims: int, additional noise\n",
    "        :param mean_val:\n",
    "        :param std_val:\n",
    "        :param noise: str or None\n",
    "        :param noise_mean: int or np.array\n",
    "        :param noise_sigma: int or np.array\n",
    "        :param n_training: int, number of training examples\n",
    "        :param redundancy_amount, percentage of redundant features, scenario 4 only\n",
    "        \"\"\"\n",
    "        self.scenario = scenario\n",
    "        self.original_dims = original_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.additional_dims = additional_dims\n",
    "        self.mean_val = mean_val\n",
    "        self.std_val = std_val\n",
    "        self.noise = noise\n",
    "        self.noise_mean = noise_mean\n",
    "        self.noise_sigma = noise_sigma\n",
    "        self.n_training = n_training\n",
    "        self.redundancy_amount = redundancy_amount\n",
    "\n",
    "\n",
    "class Experiment(object):\n",
    "    \"\"\"\n",
    "    This class represents your experiment.\n",
    "    It includes all the classes above and some general\n",
    "    information about the experiment index.\n",
    "    IF YOU ADD ANOTHER CLASS, MAKE SURE TO INCLUDE IT HERE.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 output_path,\n",
    "                 train_completed=False,\n",
    "                 hyper=None,\n",
    "                 dataset=None):\n",
    "        \"\"\"\n",
    "        :param id: index of output data folder\n",
    "        :param output_path: output directory\n",
    "        :param train_completed: bool, it indicates if the experiment has already been trained\n",
    "        :param hyper: instance of Hyperparameters class\n",
    "        :param dataset: instance of Dataset class\n",
    "        \"\"\"\n",
    "        if hyper is None:\n",
    "            hyper = Hyperparameters()\n",
    "        if dataset is None:\n",
    "            dataset = Dataset()\n",
    "\n",
    "        self.id = id\n",
    "        self.output_path = output_path\n",
    "        self.train_completed = train_completed\n",
    "        self.hyper = hyper\n",
    "        self.dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(id=0, output_path='./exp_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.dataset.additional_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\" Class for the data set generation. We consider three scenarios = [1,2,4].\n",
    "    Each related to a different transformation of the low dimensional data.\n",
    "    We generate DatasetGenerator objects everytime we generate a model and create\n",
    "    a sample split.\n",
    "\n",
    "    In the case of redundant transformation\n",
    "        ** pass the linear transformation as a dct_kwargs['A'] argument **\n",
    "\n",
    "    The risk otherwise is to have three different linear transformation for the\n",
    "    training, validation, and test dataset splits.\n",
    "    Given the input argument, the class initialization already generate the input\n",
    "    output relations, with the transformations of interest.\n",
    "\n",
    "    If the noise mean and standard deviations are not specified and we are in scenario\n",
    "    1 or 4, we generate normally distributed features.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 data_path=None,\n",
    "                 load=False,\n",
    "                 dct_dataset=None,  # dataset_spec['dataset_1']\n",
    "                 exp=None):\n",
    "        \"\"\"\n",
    "        Generate dataset for a supervised learning task. The features are extracted using\n",
    "        Gaussian distributions.\n",
    "        :param dct_dataset: dict, containing the meaningful information to generate the dataset\n",
    "        :param load: bool, if True we load the data already generated\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.load = load\n",
    "        self.dct_dataset = dct_dataset\n",
    "        self.exp = exp\n",
    "        \n",
    "        self.minimal_dataset=False\n",
    "        self.splits_lst = ['train', 'validation', 'test']\n",
    "        \n",
    "        if self.dct_dataset is not None:\n",
    "            self.p = self.dct_dataset['original_dims']\n",
    "            self.K = self.dct_dataset['output_dims']\n",
    "            self.max_add_p = self.dct_dataset['max_additional_dims']\n",
    "            self.N_per_class = self.dct_dataset['n_samples_per_class']\n",
    "            self.N = self.K * self.N_per_class\n",
    "            self.mu_array = np.array(dct_dataset['mean_val'])\n",
    "            self.sigma_array = np.array(dct_dataset['std_val'])\n",
    "\n",
    "            if not self.load:\n",
    "                self.save_minimal_data()\n",
    "                self.minimal_data = True\n",
    "\n",
    "        if load:\n",
    "            if self.data_path is None:\n",
    "                raise ValueError(\"You need to provide a path to the dataset\")\n",
    "            else:\n",
    "                # TESTED\n",
    "                self.load_minimal_data()\n",
    "                self.minimal_dataset = True\n",
    "                                              \n",
    "        if exp is not None:\n",
    "            self.exp = exp\n",
    "            \n",
    "    def _generate_minimal_data(self):\n",
    "        \"\"\" Here we generate the data by using the relevant features only.\n",
    "        Each feature is Gaussian distributed. Mean and standard\n",
    "        deviation for each variable varies depending on the user specification.\n",
    "\n",
    "        The generic i-th feature is x_i\n",
    "                    x_i = mean_i + N(0,1) * std_i, x_i in R^n_samples\n",
    "\n",
    "        The labels are generating depending on the learning task.\n",
    "        The classifier the two distribution are\n",
    "        given different values. # at the moment we are not considering the\n",
    "        multi-classification task.\n",
    "        \"\"\"\n",
    "        check_output_mu, check_input_mu = np.squeeze(np.array(self.mu_array)).shape\n",
    "        check_output_st, check_input_st = np.squeeze(np.array(self.sigma_array)).shape\n",
    "        if check_output_mu != self.K or check_output_st != self.K:\n",
    "            raise ValueError(\"Arrays inconsistent with the number of classes\")\n",
    "\n",
    "        X_ = np.zeros((self.p, self.N))\n",
    "        y_ = np.zeros((self.K, self.N))\n",
    "        for k_, (mu_class_, sigma_class_) in enumerate(zip(self.mu_array, self.sigma_array)):  # for each class\n",
    "            first_ = k_ * self.N_per_class  # n_per_class\n",
    "            last_ = self.N if k_ == self.K - 1 else (k_ + 1) * self.N_per_class\n",
    "            for id_, (mu_, sigma_) in enumerate(zip(mu_class_, sigma_class_)):\n",
    "                X_[id_, first_:last_] = mu_ + np.random.randn(last_ - first_) * sigma_\n",
    "            y_[k_, first_:last_] = 1\n",
    "\n",
    "        self.y = y_\n",
    "        self.X = X_\n",
    "        self.minimal_dataset = True\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def load_minimal_data(self):\n",
    "        # TESTED\n",
    "        self.A = np.load(join(self.data_path, 'A.npy'))\n",
    "        X_splits, y_splits, noise_splits = [], [], []\n",
    "        for fold_ in self.splits_lst:\n",
    "            X_splits.append(np.load(join(self.data_path, fold_, 'X.npy')))\n",
    "            y_splits.append(np.load(join(self.data_path, fold_, 'y.npy')))\n",
    "            noise_splits.append(np.load(join(self.data_path, fold_, 'N.npy')))\n",
    "        self.X_splits = X_splits\n",
    "        self.y_splits = y_splits\n",
    "        self.noise_splits = noise_splits                                                           \n",
    "                            \n",
    "    def save_minimal_data(self):\n",
    "        # TESTED\n",
    "        X_splits, y_splits, noise_splits = [], [], []\n",
    "        self.A = np.random.randn(self.max_add_p, self.p)\n",
    "        np.save(join(self.data_path, 'A.npy'), self.A)\n",
    "                                    \n",
    "        for id_split_, fold_ in enumerate(self.splits_lst):           \n",
    "            fold_data = join(self.data_path, fold_)             \n",
    "            os.makedirs(fold_data, exist_ok=True)             \n",
    "            self._generate_minimal_data()\n",
    "                                    \n",
    "            self.noise = np.random.randn(self.max_add_p, self.N)\n",
    "            np.save(join(self.data_path, fold_, 'X.npy'), self.X)\n",
    "            np.save(join(self.data_path, fold_, 'y.npy'), self.y)\n",
    "            np.save(join(self.data_path, fold_, 'N.npy'), self.noise)        \n",
    "    \n",
    "                            \n",
    "    def add_redundancy(self):\n",
    "        # TESTED\n",
    "        \"\"\" We add redundancy to the dataset.\n",
    "        Using a linear combination of the input features.\n",
    "        \"\"\"\n",
    "        self.A = self.A[:self.exp.dataset.additional_dims, :]\n",
    "        X_splits_ = []\n",
    "        for x_data_, f_ in zip(self.X_splits, \n",
    "                               self.splits_lst):\n",
    "            X_splits_.append(np.vstack((x_data_, \n",
    "                                        np.dot(self.A, x_data_))))\n",
    "        return X_splits_, self.y_splits                                       \n",
    "                            \n",
    "    def add_gaussian_noise(self):\n",
    "        # TESTED\n",
    "        \"\"\" We add noisy features to the dataset.\n",
    "        This is done by adding Gaussian distributed\n",
    "        random variables to the original features.\n",
    "        \"\"\"\n",
    "        if not self.minimal_dataset:\n",
    "            raise ValueError(\"Generate the dataset first\")\n",
    "        X_splits_ = []\n",
    "        for x_data_, n_data_, f_ in zip(self.X_splits, \n",
    "                                        self.noise_splits, \n",
    "                                        self.splits_lst):\n",
    "            X_splits_.append(np.vstack((x_data_, \n",
    "                                        n_data_[:self.exp.dataset.additional_dims])))\n",
    "        return X_splits_, self.y_splits\n",
    "                       \n",
    "    \n",
    "    def add_mixture(self, n_noise_feat, n_rdndt_feat):\n",
    "        # TESTED\n",
    "        \"\"\" With this call we add a percentage of redundancy and a (1-percentage) of noisy features. \"\"\"\n",
    "        self.A = self.A[:n_rdndt_feat, :]  # the first n_rdndt components\n",
    "        X_splits_ = []  # we have the three splits \n",
    "        for x_data_, n_data_, f_ in zip(self.X_splits, \n",
    "                                        self.noise_splits, \n",
    "                                        self.splits_lst):\n",
    "            tmp_ = np.vstack((x_data_, n_data_[:n_noise_feat]))\n",
    "            X_splits_.append(np.vstack((tmp_, np.dot(self.A, x_data_))))\n",
    "            \n",
    "        return X_splits_, self.y_splits\n",
    "  \n",
    "    def _get_n_train_elements_per_class(self):\n",
    "        # TESTED\n",
    "        \"\"\" Consider a fixed amount of training data. \"\"\"\n",
    "        if not self.minimal_data or self.exp is None:\n",
    "            raise ValueError(\"Generate the dataset first\")\n",
    "        n_per_class = self.exp.dataset.n_training\n",
    "\n",
    "        y_tr = self.y_splits[0]  # (k, n)\n",
    "        n_classes, n_samples = y_tr.shape\n",
    "        n_s_per_class = data_generator.exp.dataset.n_training\n",
    "\n",
    "        idx = np.array([], dtype=int)\n",
    "        for k in range(n_classes):\n",
    "            idx = np.append(idx, np.arange(k * (n_samples // n_classes), \n",
    "                                           k * (n_samples // n_classes) + n_s_per_class))\n",
    "        return idx\n",
    "                            \n",
    "    def generate_input_experiment(self):\n",
    "        # TESTED\n",
    "        \"\"\" Generate the dataset (X, y) for a specific experiment. \"\"\"        \n",
    "        if self.exp.dataset.scenario == 1:\n",
    "            return self.add_gaussian_noise()\n",
    "                            \n",
    "        elif self.exp.dataset.scenario == 2:\n",
    "            return self.add_redundancy()\n",
    "                            \n",
    "        elif self.exp.dataset.scenario == 4:\n",
    "            r_ = self.exp.dataset.redundancy_amount \n",
    "            n_noise_feat = int(self.exp.dataset.additional_dims * (1-r_))\n",
    "            n_rdndt_feat = int(self.exp.dataset.additional_dims * r_)\n",
    "            return self.add_mixture(n_noise_feat, n_rdndt_feat)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_dataset = 'dataset_1'\n",
    "\n",
    "data_generator = DatasetGenerator(data_path=key_dataset, \n",
    "                                  #Â dct_dataset=dataset_spec[key_dataset],\n",
    "                                  load=True,\n",
    "                                  exp=exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here\n",
      "I am here\n"
     ]
    }
   ],
   "source": [
    "[X_splits, y_splits] = data_generator.generate_input_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9, 5000,\n",
       "       5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12227757,  0.71764277,  0.77043894, -0.4033331 ,  0.75041773,\n",
       "         0.5774073 , -0.78842729, -0.36212822,  0.3605753 , -0.87174395,\n",
       "         0.88089455,  0.62452585, -0.38670337, -0.37459276,  0.92215839,\n",
       "        -0.01694338,  0.5577367 ,  0.10868146, -0.06597793,  0.2806442 ],\n",
       "       [ 2.34519917,  2.64982012,  2.07982889,  1.62859002,  1.93802655,\n",
       "         1.39984328,  1.73107201,  2.34039206,  2.27191495,  1.94834235,\n",
       "        -0.91988174, -1.26135136, -1.27760393, -2.2726067 , -2.18425958,\n",
       "        -1.87086354, -1.21286184, -1.51026562, -1.95748186, -1.52044892],\n",
       "       [-1.07952103, -1.34703513, -1.63130437, -0.96195073, -1.4177742 ,\n",
       "        -1.84969245, -1.20151081, -1.78456528, -1.86380838, -1.62562871,\n",
       "        -2.47138789, -1.90623784, -3.15547536, -4.01583724, -2.66289881,\n",
       "        -2.82092555, -2.95659019, -3.06084798, -1.74584754, -3.40076646],\n",
       "       [ 2.94611934,  3.5126783 ,  4.13466964,  4.52262513,  3.66256933,\n",
       "         3.60111639,  3.82307553,  3.81750755,  3.94609627,  4.36362639,\n",
       "        -3.96881292, -3.94531096, -4.14067068, -4.32231841, -3.70300353,\n",
       "        -4.25104682, -4.16911371, -4.02309742, -3.09186645, -3.94877993],\n",
       "       [-1.89232372, -2.46199754, -1.66558518, -1.56757851, -2.5885283 ,\n",
       "        -1.11331974, -1.83374214, -2.83387201, -1.14648275, -2.66240661,\n",
       "         1.05045423,  0.66283106,  0.76270067,  1.22527235,  1.10634739,\n",
       "         1.41621995,  0.26185531,  0.79769225,  0.19910798,  0.33009565],\n",
       "       [-0.47583997, -1.47355322, -2.6775891 , -0.25253519, -0.93250807,\n",
       "        -0.96880544, -1.61414511, -0.82747197, -1.58752955, -2.14892607,\n",
       "         0.32175716, -0.56759224, -0.24622351, -0.75276671, -0.62267164,\n",
       "        -0.93921816, -1.86462504, -0.89835365, -0.075099  , -1.16976863],\n",
       "       [ 1.84704396,  2.5535502 ,  2.57687502,  3.51022859,  2.37813163,\n",
       "         2.47104614,  2.63818185,  3.31921447,  3.32704624,  2.23084663,\n",
       "        -0.47380645,  0.48243261,  0.08837161, -0.18583805,  0.2669895 ,\n",
       "        -0.38156091,  0.21333888, -0.32491502, -1.38361082, -0.87389258],\n",
       "       [-0.35291547,  0.09251895, -0.13775793,  0.14307001,  0.63732072,\n",
       "         0.77497917,  0.4840764 ,  0.13230266, -0.55387913,  1.06669807,\n",
       "         0.2188241 , -0.10455285,  0.71597779, -0.49312374,  1.20826915,\n",
       "         0.56080848, -0.71312481, -0.22632966,  0.02508452,  1.03577455],\n",
       "       [ 1.51946314,  1.97134816,  0.81878975,  1.26581122,  1.29738932,\n",
       "         1.41640949,  0.78114355,  1.73242947,  1.5803729 ,  2.22229367,\n",
       "         2.12348775,  2.87803266,  2.05303797,  2.68133416,  2.59653506,\n",
       "         2.14946607,  2.14509259,  2.05851546,  2.03857848,  2.00044399],\n",
       "       [-0.47271202, -0.92881491, -0.11119624,  0.45136378, -0.83386298,\n",
       "         0.30882011, -0.33168409, -0.80032386,  1.31876711,  0.12673692,\n",
       "        -1.58086141, -1.42832741, -1.47255857, -1.28449293, -1.34623866,\n",
       "        -1.63685682, -1.66761502, -1.14502957, -0.70805866, -1.7445205 ],\n",
       "       [ 0.40767284,  1.08382702,  1.75778152,  1.84901437,  2.24048612,\n",
       "         2.21696414,  2.45100358,  1.99934982,  1.44685086,  2.22264178,\n",
       "        -0.45925846, -1.26234568, -0.69018212, -0.61351915, -1.80650961,\n",
       "        -1.41308695, -1.09960914, -1.18523711, -1.06319697, -1.56528033],\n",
       "       [ 0.48395877,  1.54229789,  1.66254395, -0.21598224,  0.79846395,\n",
       "         0.79259295,  0.14374269,  0.69040298,  1.12197303,  0.20606136,\n",
       "         4.45359397,  3.397884  ,  3.16111065,  3.73505945,  2.58206775,\n",
       "         2.49954571,  2.44518311,  3.5404228 ,  3.99257686,  2.8824473 ],\n",
       "       [-1.08473005, -3.02803872, -1.75384817, -2.06786318, -1.95392939,\n",
       "        -1.1381863 , -1.81369768, -2.53270476, -1.61831921, -2.85165803,\n",
       "         3.03947361,  3.47037632,  3.13723158,  3.30376717,  2.86372526,\n",
       "         3.21404094,  3.54973019,  3.03227034,  2.94213847,  3.30792808],\n",
       "       [-1.04057784, -0.96549637, -0.98201889, -0.66125688, -1.09097742,\n",
       "        -1.76982232, -0.41975152, -0.89433777, -0.77276545, -0.53192189,\n",
       "        -0.41064479, -0.82478367, -0.31520633, -0.33290653,  0.21099115,\n",
       "        -1.03299051, -0.23485427, -1.01211722, -0.3023832 , -0.36242055],\n",
       "       [ 0.32066508,  0.22166493, -0.79646204, -0.12637649, -0.04605451,\n",
       "         0.61408463,  0.89253889,  0.23785718, -0.12762304,  0.41455887,\n",
       "         1.97059496,  2.18135204,  2.12959834,  2.04344214,  1.92289819,\n",
       "         2.52438709,  2.24127971,  2.96618245,  2.46308209,  2.05115933],\n",
       "       [-4.40510269, -4.18939275, -4.9792063 , -4.02454923, -4.20101048,\n",
       "        -4.92653021, -4.57048623, -4.99605125, -4.57492691, -5.18558038,\n",
       "        -0.29702427,  0.91613962,  0.21229549,  0.30438003,  0.38464834,\n",
       "         0.17433003, -0.37116601, -1.1068259 , -0.1435276 ,  0.07392735],\n",
       "       [ 0.46283708,  1.57117558,  0.92258521,  1.71817002,  1.26308827,\n",
       "         0.94792347,  0.01234616,  0.78519053,  1.26881532,  1.15878475,\n",
       "         0.13594393,  1.07165186,  0.27519855,  0.42873714, -0.47679297,\n",
       "        -0.69918144, -0.68891717,  0.54514178,  0.5642609 , -0.15284906],\n",
       "       [-1.31965532, -1.49083861, -2.45826382, -1.4028612 , -1.08686502,\n",
       "        -1.04407071, -0.55489794, -1.12598845, -2.21301508, -1.5160555 ,\n",
       "        -1.6165315 , -1.74429793, -0.75272508, -1.33037137, -1.8112474 ,\n",
       "        -2.48000349, -1.42996284, -2.26310798, -1.81676979, -2.33197457],\n",
       "       [-1.22662766, -0.80482204, -0.80219649, -1.15572128, -0.89911989,\n",
       "        -1.45822854, -1.04807783, -1.28098914, -1.53753639, -1.39127773,\n",
       "         0.81860442, -0.42469145, -0.30608889,  0.07630369,  1.16216543,\n",
       "         0.26402974,  0.5110606 ,  0.25415746,  0.95473268,  0.4419837 ],\n",
       "       [ 0.52972983,  2.2572669 ,  1.93737841,  0.68563812,  0.33868077,\n",
       "         1.09017069,  1.25416949,  1.05265112,  2.28261863,  0.92795701,\n",
       "         0.89297845,  0.54446753,  1.12945169,  0.61443941,  0.88430148,\n",
       "         1.12662428,  2.04662826,  1.52756728,  1.62848151,  0.93198731],\n",
       "       [-3.72952844, -3.61522022, -2.96231827, -3.17251611, -2.85989328,\n",
       "        -3.89574212, -3.30588882, -3.84027623, -3.65156823, -3.09957825,\n",
       "         0.52928375,  0.97095105,  1.17433438,  0.99426616,  0.62206113,\n",
       "         0.61934621,  0.08848243,  1.07883958,  0.11303723,  0.79654033],\n",
       "       [ 2.18506729,  2.07060913,  1.90239877,  2.83402187,  3.46393484,\n",
       "         2.13266366,  2.10513969,  2.88182057,  2.81106283,  2.39382867,\n",
       "        -1.12736847,  0.54035067, -0.63299885, -0.13335475,  0.38703819,\n",
       "         1.08960915,  1.15332595, -0.03495858, -0.14451386,  0.07660833],\n",
       "       [ 0.20143986,  0.03870306,  0.8157614 ,  0.47913053,  0.93121534,\n",
       "         0.03120484,  0.42992358, -0.32245441,  0.39504753,  0.69677259,\n",
       "         1.89702973,  1.22419694,  0.90670316,  1.77491522,  1.11739315,\n",
       "         1.32728614,  2.2395772 ,  2.33049661,  2.70090278,  2.02917595],\n",
       "       [-1.15124921, -0.11027451, -1.46841005, -0.23156673, -1.65619101,\n",
       "        -0.00740088, -0.6526012 , -0.4391913 , -0.01252201, -2.21805142,\n",
       "        -3.858867  , -3.37257163, -2.88351689, -2.97762084, -3.95746836,\n",
       "        -3.46885248, -3.42811523, -3.49234525, -2.75099953, -3.33984446],\n",
       "       [-0.93835501, -0.76789486, -1.37274661, -0.92696779, -0.79940685,\n",
       "        -2.32817386, -0.92431704, -1.01473992, -0.64102728, -0.91027964,\n",
       "         3.81377365,  2.87453371,  3.15870435,  3.20280367,  3.1590955 ,\n",
       "         2.92644192,  3.44018186,  3.64241754,  2.91145613,  3.02716587],\n",
       "       [-2.84311391, -2.75238537, -2.61938555, -3.20152221, -2.81750318,\n",
       "        -2.2433402 , -3.13646136, -3.08047657, -1.98667606, -2.85085775,\n",
       "        -0.50962501, -0.37093557,  0.46940472,  0.07796811,  0.35661635,\n",
       "         0.23956927,  0.02072784,  0.9247674 ,  0.46516798,  0.18521234],\n",
       "       [-3.90209876, -2.87565295, -2.66417172, -3.2413195 , -2.89544554,\n",
       "        -2.23441619, -3.18166093, -3.22946354, -2.36706693, -3.09164944,\n",
       "         2.23539725,  1.55725475,  1.63835184,  2.16677623,  0.79487719,\n",
       "         2.07076388,  1.98030905,  2.01817759,  1.69346328,  1.81617346],\n",
       "       [-2.89991306, -2.84431312, -3.0753888 , -3.47243402, -3.69639826,\n",
       "        -3.58566182, -3.59530727, -2.9698446 , -3.38032584, -2.90937304,\n",
       "        -2.31624082, -2.26995683, -2.0295744 , -0.82127086, -2.09170008,\n",
       "        -3.16788256, -2.35948869, -2.50758913, -2.30527729, -1.95778853],\n",
       "       [ 0.85508735, -0.00842955,  0.03051598,  0.36558537,  0.84141123,\n",
       "         1.32945647,  1.06951241,  0.48082742,  1.22417286,  1.67013246,\n",
       "         0.77025022,  0.40913821,  0.85108851,  0.67679724,  1.25646132,\n",
       "         1.36792703,  1.11678158,  1.33077025,  1.01411703,  1.80476962],\n",
       "       [ 1.09003799,  1.21403982, -0.33708836,  0.49185606,  1.27510054,\n",
       "         1.08687987,  0.85291066,  1.4714412 ,  0.19152323,  0.25215267,\n",
       "         2.82306641,  2.88755683,  0.89227211,  2.13959943,  2.48884172,\n",
       "         2.3905806 ,  2.49789357,  1.31959623,  2.26259512,  2.18713478],\n",
       "       [ 0.23854865, -0.73885276,  0.06238762, -0.07592416, -0.15601142,\n",
       "        -0.16542304,  0.73221751, -0.08597703, -0.22856105,  0.62855282,\n",
       "        -0.13028668, -0.87598068, -0.46966761,  0.16126637, -0.25998081,\n",
       "         0.39909103,  0.03370614,  1.5076167 ,  2.61493803, -1.09452155],\n",
       "       [ 0.99138832, -1.21081723, -0.29866784,  0.2831277 ,  0.60335561,\n",
       "         1.55476684,  2.06714387, -0.24247171, -0.66909765, -0.15523371,\n",
       "        -0.21781461, -1.61061982, -0.24951724, -0.75795607, -0.70094241,\n",
       "         0.19156774, -0.56876892,  0.42342885, -1.00569798, -1.13715544]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_splits[0][:, idx]\n",
    "X_splits[0][:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_splits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_splits, y_splits] = data_generator.add_mixture(n_noise_feat=1, n_rdndt_feat=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(X_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.dataset.redundancy_amount = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator.X_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[1,0,0,0,0],\n",
    "              [0,1,0,0,0],\n",
    "              [0,1,0,0,0]])\n",
    "\n",
    "np.unique(y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
